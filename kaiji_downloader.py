#!/usr/bin/env python3
"""
æ±è¨¼ä¸Šå ´ä¼æ¥­æƒ…å ±å–å¾—ã‚µãƒ¼ãƒ“ã‚¹
ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

"""

å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ä½¿ç”¨ä¾‹

    # ä¾‹1: å…¨1,681ç¤¾ã®HTMLã¨æ·»ä»˜è³‡æ–™ã‚’åé›†ï¼ˆæ¨å®š20æ™‚é–“ç¨‹åº¦ï¼‰
        uv run python kaiji_downloader.py batch-download codelist.csv --types=html,attachments --delay-min=2 --delay-max=5

    # ä¾‹2: æœ€åˆã®100ç¤¾ã®HTMLã¨æ·»ä»˜è³‡æ–™ã‚’ãƒ†ã‚¹ãƒˆåé›†
        uv run python kaiji_downloader.py batch-download codelist.csv --types=html,attachments --max=100 --delay-min=2 --delay-max=5

    # ä¾‹3: 500è¡Œç›®ã‹ã‚‰å†é–‹ã—ã¦HTMLã¨æ·»ä»˜è³‡æ–™ã‚’åé›†
        uv run python kaiji_downloader.py batch-download codelist.csv --types=html,attachments --resume=500 --delay-min=2 --delay-max=5


å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ä¸€è¦§

codelist.csvã‹ã‚‰å–å¾—ã™ã‚‹å ´åˆ

    # 1. XBRLã®ã¿é«˜é€Ÿåé›†
        uv run python kaiji_downloader.py batch-download codelist.csv --types=xbrl --delay-min=2 --delay-max=5
    # 2. HTMLã‚µãƒãƒªãƒ¼åé›†
        uv run python kaiji_downloader.py batch-download codelist.csv --types=html --delay-min=2 --delay-max=5
    # 3. æ·»ä»˜è³‡æ–™åé›†
        uv run python kaiji_downloader.py batch-download codelist.csv --types=attachments --delay-min=2 --delay-max=5

"""

import json
import csv
import os
from datetime import datetime
from src.scraper import JPXScraper


def test_single_company(stock_code: str = "9984", debug: bool = False, fetch_disclosure: bool = False, 
                       download_xbrl: bool = False, download_html: bool = False, download_attachments: bool = False):
    """
    å˜ä¸€ä¼æ¥­ï¼ˆã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯: 9984ï¼‰ã§ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    
    Args:
        stock_code: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆHTMLã‚’ä¿å­˜ï¼‰
        fetch_disclosure: é©æ™‚é–‹ç¤ºæƒ…å ±ã‚‚å–å¾—ã™ã‚‹ã‹
        download_xbrl: XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
        download_html: HTMLã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
        download_attachments: æ·»ä»˜è³‡æ–™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
    """
    print(f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {stock_code} ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
    print("-" * 50)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼‰
    scraper = JPXScraper(debug=debug)
    
    # ä¼æ¥­æƒ…å ±ã‚’å–å¾—
    company_info = scraper.search_company(stock_code)
    
    if company_info:
        print("å–å¾—æˆåŠŸï¼")
        print("\nã€å–å¾—ã—ãŸä¼æ¥­æƒ…å ±ã€‘")
        for key, value in company_info.items():
            if value:
                print(f"  {key}: {value}")
        
        # é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—
        disclosure_docs = []
        if fetch_disclosure:
            print("\nã€é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—ä¸­...ã€‘")
            disclosure_docs = scraper.fetch_disclosure_documents(stock_code)
            
            if disclosure_docs:
                print(f"\nã€é©æ™‚é–‹ç¤ºæƒ…å ±ã€‘ {len(disclosure_docs)}ä»¶")
                # æœ€æ–°ã®5ä»¶ã‚’è¡¨ç¤º
                for i, doc in enumerate(disclosure_docs[:5], 1):
                    print(f"\n{i}. {doc['date']} - {doc['title']}")
                    if doc['pdf_url']:
                        print(f"   PDF: {doc['pdf_url']}")
                    if doc['xbrl_url']:
                        print(f"   XBRL: {doc['xbrl_url']}")
                    if doc['html_summary_url']:
                        print(f"   HTMLã‚µãƒãƒªãƒ¼: {doc['html_summary_url']}")
                    if doc['attachments']:
                        print(f"   æ·»ä»˜è³‡æ–™: {len(doc['attachments'])}ä»¶")
                
                # æ±ºç®—çŸ­ä¿¡ã®ã¿ã‚’æŠ½å‡º
                kessan_docs = [doc for doc in disclosure_docs if 'æ±ºç®—çŸ­ä¿¡' in doc['title']]
                if kessan_docs:
                    print(f"\nã€æ±ºç®—çŸ­ä¿¡ã€‘ {len(kessan_docs)}ä»¶")
                    for doc in kessan_docs[:3]:
                        print(f"  {doc['date']} - {doc['title']}")
                        if doc['xbrl_url']:
                            print(f"    XBRL: {doc['xbrl_url']}")
                
                # XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if download_xbrl:
                    print("\nã€XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...ã€‘")
                    download_results = scraper.download_xbrl_files(
                        disclosure_docs,
                        download_dir=f"downloads/xbrl/{stock_code}",
                        stock_code=stock_code
                    )
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’è¡¨ç¤º
                    successful_downloads = [r for r in download_results if r['status'] == 'success']
                    if successful_downloads:
                        print(f"\nã€XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã€‘ {len(successful_downloads)}ä»¶")
                        for result in successful_downloads[:3]:
                            print(f"  {result['local_file']} ({result['file_size']:,} bytes)")
                
                # HTMLã‚µãƒãƒªãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if download_html:
                    print("\nã€HTMLã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...ã€‘")
                    download_results = scraper.download_html_summaries(stock_code)
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’è¡¨ç¤º
                    successful_downloads = [r for r in download_results if r['status'] == 'success']
                    if successful_downloads:
                        print(f"\nã€HTMLã‚µãƒãƒªãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã€‘ {len(successful_downloads)}ä»¶")
                        for result in successful_downloads[:3]:
                            print(f"  {result['local_file']} ({result['file_size']:,} bytes)")
                
                # æ·»ä»˜è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if download_attachments:
                    print("\nã€æ·»ä»˜è³‡æ–™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...ã€‘")
                    download_results = scraper.download_attachments(stock_code)
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’è¡¨ç¤º
                    successful_downloads = [r for r in download_results if r['status'] == 'success']
                    if successful_downloads:
                        print(f"\nã€æ·»ä»˜è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã€‘ {len(successful_downloads)}ä»¶")
                        for result in successful_downloads[:3]:
                            print(f"  {result['local_file']} ({result['file_size']:,} bytes)")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/{stock_code}_{timestamp}.json"
        
        result_data = {
            'company_info': company_info,
            'disclosure_documents': disclosure_docs
        }
        
        os.makedirs("data", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nçµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return True
    else:
        print("æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return False


def batch_download_all(
    csv_file: str = "codelist.csv",
    download_types: list = None,
    resume_from: int = 0,
    max_companies: int = None,
    delay_seconds: int = 3,
    delay_min: int | None = None,
    delay_max: int | None = None,
):
    """
    codelist.csvã‹ã‚‰å…¨éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    
    Args:
        csv_file: éŠ˜æŸ„ãƒªã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«
        download_types: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡ã®ãƒªã‚¹ãƒˆ
        resume_from: å†é–‹ã™ã‚‹è¡Œç•ªå·
        max_companies: æœ€å¤§å‡¦ç†ä¼æ¥­æ•°
        delay_seconds: ä¼æ¥­é–“å¾…æ©Ÿç§’æ•°
    """
    print(f"ğŸš€ å…¨éŠ˜æŸ„ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
    print(f"ğŸ“‹ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    print("-" * 50)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    scraper = JPXScraper(debug=False)
    
    # ãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    results = scraper.download_all_files_batch(
        codelist_csv=csv_file,
        download_types=download_types,
        resume_from=resume_from,
        max_companies=max_companies,
        delay_seconds=delay_seconds,
        delay_min=delay_min,
        delay_max=delay_max,
    )
    
    return results


def process_batch(csv_file: str = "codelist.csv"):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒƒãƒå‡¦ç†
    
    Args:
        csv_file: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®CSVãƒ•ã‚¡ã‚¤ãƒ«
    """
    if not os.path.exists(csv_file):
        print(f"ã‚¨ãƒ©ãƒ¼: {csv_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"{csv_file} ã‹ã‚‰ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹...")
    print("-" * 50)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    scraper = JPXScraper()
    
    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    results = []
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        stock_codes = []
        
        # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚«ãƒ©ãƒ ã‚’æ¢ã™
        if reader.fieldnames:
            code_column = None
            for field in reader.fieldnames:
                if 'code' in field.lower() or 'è¨¼åˆ¸' in field:
                    code_column = field
                    break
            
            if code_column:
                for row in reader:
                    stock_codes.append(row[code_column])
            else:
                # ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
                f.seek(0)
                next(reader)  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                for row in reader:
                    stock_codes.append(list(row.values())[0])
    
    # å„è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—
    total = len(stock_codes)
    success_count = 0
    
    for i, code in enumerate(stock_codes, 1):
        print(f"\n[{i}/{total}] è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {code} ã‚’å‡¦ç†ä¸­...")
        
        company_info = scraper.search_company(code)
        
        if company_info:
            results.append(company_info)
            success_count += 1
            print(f"  â†’ æˆåŠŸ: {company_info.get('company_name', 'ä¸æ˜')}")
        else:
            print(f"  â†’ å¤±æ•—")
        
        # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
        if i < total:
            import time
            time.sleep(2)  # 2ç§’å¾…æ©Ÿ
    
    # çµæœã‚’ã¾ã¨ã‚ã¦ä¿å­˜
    if results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/batch_result_{timestamp}.json"
        
        os.makedirs("data", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\n\nå‡¦ç†å®Œäº†ï¼")
        print(f"æˆåŠŸ: {success_count}/{total} ä»¶")
        print(f"çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "batch":
        # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
        csv_file = sys.argv[2] if len(sys.argv) > 2 else "codelist.csv"
        process_batch(csv_file)
    elif len(sys.argv) > 1 and sys.argv[1] == "debug":
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=True, fetch_disclosure=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "disclosure":
        # é©æ™‚é–‹ç¤ºæƒ…å ±å–å¾—ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=False, fetch_disclosure=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "xbrl":
        # XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=True, fetch_disclosure=True, download_xbrl=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "html":
        # HTMLã‚µãƒãƒªãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=True, fetch_disclosure=True, download_html=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "attachments":
        # æ·»ä»˜è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=True, fetch_disclosure=True, download_attachments=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "all":
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        stock_code = sys.argv[2] if len(sys.argv) > 2 else "99840"
        test_single_company(stock_code, debug=True, fetch_disclosure=True, 
                           download_xbrl=True, download_html=True, download_attachments=True)
    elif len(sys.argv) > 1 and sys.argv[1] == "batch-download":
        # å…¨éŠ˜æŸ„ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        csv_file = sys.argv[2] if len(sys.argv) > 2 else "codelist.csv"
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æ
        download_types = None
        resume_from = 0
        max_companies = None
        delay_seconds = 3
        delay_min = None
        delay_max = None
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
        for i, arg in enumerate(sys.argv[3:], 3):
            if arg.startswith("--types="):
                types_str = arg.split("=")[1]
                download_types = types_str.split(",")
            elif arg.startswith("--resume="):
                resume_from = int(arg.split("=")[1])
            elif arg.startswith("--max="):
                max_companies = int(arg.split("=")[1])
            elif arg.startswith("--delay="):
                delay_seconds = int(arg.split("=")[1])
            elif arg.startswith("--delay-min="):
                delay_min = float(arg.split("=")[1])
            elif arg.startswith("--delay-max="):
                delay_max = float(arg.split("=")[1])
        # ç¯„å›²æŒ‡å®šã®æ­£è¦åŒ–
        if delay_min is None and delay_max is None:
            # æ—§ã‚ªãƒ—ã‚·ãƒ§ãƒ³(--delay)ã®ã¿æŒ‡å®šæ™‚ã¯å›ºå®šå¾…æ©Ÿ
            delay_min = delay_seconds
            delay_max = delay_seconds
        elif delay_min is None:
            delay_min = delay_max
        elif delay_max is None:
            delay_max = delay_min
        # å…¥åŠ›ãŒé€†ã®å ´åˆã¯å…¥ã‚Œæ›¿ãˆ
        if delay_min > delay_max:
            delay_min, delay_max = delay_max, delay_min

        batch_download_all(
            csv_file,
            download_types,
            resume_from,
            max_companies,
            delay_seconds,
            delay_min,
            delay_max,
        )
    elif len(sys.argv) > 1 and sys.argv[1] == "batch-download-test":
        # ãƒ†ã‚¹ãƒˆç”¨ï¼šæœ€åˆã®5ç¤¾ã®ã¿å‡¦ç†
        csv_file = sys.argv[2] if len(sys.argv) > 2 else "codelist.csv"
        batch_download_all(csv_file, max_companies=5, delay_seconds=1)
    else:
        # å˜ä¸€ä¼æ¥­ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 99840ï¼‰
        stock_code = sys.argv[1] if len(sys.argv) > 1 else "99840"
        test_single_company(stock_code)
