# import_html_summary.py 使用手順書

## 概要
`import_html_summary.py`は、HTML要約データのCSVファイルをSQLiteデータベースに自動インポートするプログラムです。既存データとの重複を自動的にチェックし、新規データのみを追加します。

## 必要要件
- Python 3.11 以上（標準ライブラリのみ使用）
- 同一フォルダ内に以下のファイルが必要：
  - `html_summary.csv`：インポート元のCSVファイル
  - `html_summary.db`：インポート先のSQLiteデータベース

## 使用方法

### 基本的な実行方法
```bash
# プログラムが存在するフォルダに移動
cd /path/to/your/folder/

# 実行（Python3を明示的に指定）
python3 import_html_summary.py
```

### 実行例
```bash
# 例1: outputフォルダでの実行
cd /home/jptyf/workspace/jpx_kaiji_service/output/
python3 import_html_summary.py

# 例2: 別の場所にコピーして実行
cp import_html_summary.py /tmp/data/
cp html_summary.csv /tmp/data/
cp html_summary.db /tmp/data/
cd /tmp/data/
python3 import_html_summary.py
```

## 動作仕様

### 重複チェック（実装準拠）
CSVの各行とDBの既存行が「全カラム一致（NULLも含めて厳密一致）」の場合に重複と判定し、スキップします。

- 比較対象カラム（計12列）:
  - `date`, `filing_date`, `code`, `company_name`,
    `fiscal_year_end`, `quarterly_period`, `factor_tag`, `factor_jp`,
    `value`, `has_value`, `is_nil`, `data_type`
- 比較方法: SQLiteの`IS`演算子を使用し、NULL同士も一致として扱います
- 備考: `quarterly_period`は整数型に正規化して比較します

### 処理の流れ
1. **ファイル確認**：CSVファイルとデータベースファイルの存在確認
2. **データベース接続**：SQLiteデータベースへの接続と既存データ件数の確認
3. **インデックス作成**：重複チェック高速化のためのインデックス作成（初回のみ）
   - 作成インデックス: `idx_full_duplicate_check`
   - 対象カラム: `(date, code, fiscal_year_end, quarterly_period, factor_tag, filing_date, company_name)`
   - 旧インデックス `idx_duplicate_check` は存在する場合に削除されます
4. **バッチ処理**：1000件ずつデータを処理（メモリ効率化）
5. **進捗表示**：5秒ごとに処理状況を表示
6. **結果報告**：処理完了後に統計情報を表示

## 出力情報

### コンソール出力例（実際のテスト結果）
```
2025-08-27 12:32:28 - INFO - ==================================================
2025-08-27 12:32:28 - INFO - HTML Summary インポートプログラム
2025-08-27 12:32:28 - INFO - ==================================================
2025-08-27 12:32:28 - INFO - CSVファイルサイズ: 2,169,782,078 bytes
2025-08-27 12:32:28 - INFO - データベースに接続しました: html_summary.db
2025-08-27 12:32:28 - INFO - 既存データ件数: 5,664,449
2025-08-27 12:32:28 - INFO - 重複チェック用インデックスを確認/作成しました（全カラム対応版）
2025-08-27 12:32:28 - INFO - インポート処理を開始します...
2025-08-27 12:32:33 - INFO - 進捗: 処理済み 622,000 行 (新規 375,834, スキップ 246,166, エラー 0) - 124365 行/秒
2025-08-27 12:32:38 - INFO - 進捗: 処理済み 1,222,000 行 (新規 774,018, スキップ 447,982, エラー 0) - 122148 行/秒
...
# 実際の処理速度は約10万～12万行/秒
```

### ログファイル
実行時に自動的にログファイルが作成されます：
- ファイル名形式：`import_html_summary_YYYYMMDD_HHMMSS.log`
- 例：`import_html_summary_20240115_103000.log`
- 内容：コンソール出力と同じ内容が記録されます

## エラー対処法

### よくあるエラーと対処法

#### 1. CSVファイルが見つからない
```
ERROR - CSVファイルが見つかりません: html_summary.csv
```
**対処法**：`html_summary.csv`が同じフォルダにあることを確認してください。

#### 2. データベースファイルが見つからない
```
ERROR - データベースファイルが見つかりません: html_summary.db
```
**対処法**：`html_summary.db`が同じフォルダにあることを確認してください。

#### 3. テーブルが存在しない
```
ERROR - html_summaryテーブルが存在しません
```
**対処法**：データベースに`html_summary`テーブルが作成されているか確認してください。

#### 4. メモリ不足
大容量CSVファイルの処理時にメモリ不足が発生する場合があります。
**対処法**：
- バッチサイズを調整（プログラム内の`batch_size`パラメータ）
- システムのメモリを増やす
- CSVファイルを分割して処理

## 注意事項

1. **データの整合性**
   - プログラムは新規データの追加のみを行い、既存データの更新は行いません
   - 重複チェックは「全カラム一致（12列）」で行われます（NULL比較には`IS`演算子を使用）

2. **パフォーマンス**
   - 処理速度は約10万～12万行/秒（環境により変動）
   - 2GB程度のCSVファイルは数分で処理可能
   - 初回実行時はインデックス作成のため、少し時間がかかります
   - 2回目以降はインデックスが利用されるため高速化されます

3. **中断と再開**
   - Ctrl+Cで処理を安全に中断できます
   - 中断した場合、コミット済みのデータは保持されます
   - 再実行時は、既にインポートされたデータはスキップされます

4. **文字エンコーディング**
   - CSVファイルはUTF-8（BOM付き/なし両対応）である必要があります
   - 他のエンコーディングの場合は事前に変換してください

5. **CSVの必須カラム**
   - 以下の12列が必要です：
     `date, filing_date, code, company_name, fiscal_year_end, quarterly_period, factor_tag, factor_jp, value, has_value, is_nil, data_type`
   - `value`列は `data_type` が`value`の場合に数値へ自動変換を試みます（int/float）。`empty`や空文字はNULLとして扱います

## トラブルシューティング

### 処理が遅い場合
1. データベースファイルのあるディスクの空き容量を確認
2. 他のプログラムがデータベースを使用していないか確認
3. インデックスが正しく作成されているか確認

### データが追加されない場合
1. CSVファイルの形式が正しいか確認
2. 重複データではないか確認（5つのキー項目）
3. ログファイルでエラーメッセージを確認

### サポート情報
問題が解決しない場合は、以下の情報と共に報告してください：
- ログファイルの内容
- CSVファイルのサンプル（最初の数行）
- データベースのテーブル構造
- Pythonのバージョン（`python --version`）
