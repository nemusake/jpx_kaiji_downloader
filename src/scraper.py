import requests
from bs4 import BeautifulSoup
import time
import re
from typing import Dict, Optional, List
import json
import os


class JPXScraper:
    """æ±è¨¼é©æ™‚é–‹ç¤ºæƒ…å ±ã‚µã‚¤ãƒˆã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, debug: bool = False):
        self.session = requests.Session()
        self.base_url = "https://www2.jpx.co.jp/tseHpFront/"
        self.debug = debug
        self.last_response = None  # æœ€å¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿æŒ
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        if self.debug:
            os.makedirs("debug", exist_ok=True)
    
    def search_company(self, stock_code: str) -> Optional[Dict]:
        """
        è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’æ¤œç´¢ã—ã€åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¾ã§é·ç§»
        
        Args:
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: "9984"ï¼‰
            
        Returns:
            ä¼æ¥­æƒ…å ±ã®è¾æ›¸ã€ã¾ãŸã¯å–å¾—å¤±æ•—æ™‚ã¯None
        """
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
            print(f"ã‚¹ãƒ†ãƒƒãƒ—1: æ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            init_url = self.base_url + "JJK010010Action.do?Show"
            response = self.session.get(init_url)
            response.raise_for_status()
            time.sleep(1)
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ã®é€ä¿¡
            print(f"ã‚¹ãƒ†ãƒƒãƒ—2: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {stock_code} ã§æ¤œç´¢ä¸­...")
            search_url = self.base_url + "JJK010010Action.do"
            
            # ãƒ–ãƒ­ã‚°ã‚’å‚è€ƒã«ã—ãŸæ­£ã—ã„POSTãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            search_data = {
                'ListShow': 'ListShow',
                'sniMtGmnId': '',
                'dspSsuPd': '10',
                'dspSsuPdMapOut': '10>10ä»¶<50>50ä»¶<100>100ä»¶<200>200ä»¶<',
                'mgrMiTxtBx': '',  # ç©ºã«ã™ã‚‹
                'eqMgrCd': stock_code,  # ã“ã“ã«è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…¥ã‚Œã‚‹
                'szkbuChkbxMapOut': '011>ãƒ—ãƒ©ã‚¤ãƒ <012>ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰<013>ã‚°ãƒ­ãƒ¼ã‚¹<008>TOKYO PRO Market<bj1>ï¼<be1>ï¼<111>å¤–å›½æ ªãƒ—ãƒ©ã‚¤ãƒ <112>å¤–å›½æ ªã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰<113>å¤–å›½æ ªã‚°ãƒ­ãƒ¼ã‚¹<bj2>ï¼<be2>ï¼<ETF>ETF<ETN>ETN<RET>ä¸å‹•ç”£æŠ•è³‡ä¿¡è¨—(REIT)<IFD>ã‚¤ãƒ³ãƒ•ãƒ©ãƒ•ã‚¡ãƒ³ãƒ‰<999>ãã®ä»–<'
            }
            
            response = self.session.post(search_url, data=search_data)
            response.raise_for_status()
            
            if self.debug:
                with open(f"debug/{stock_code}_step2_search_result.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
            
            # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: åŸºæœ¬æƒ…å ±ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆJJK010030Action.doã¸ã®é·ç§»ï¼‰
            print(f"ã‚¹ãƒ†ãƒƒãƒ—3: åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã«é·ç§»ä¸­...")
            
            # ãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            form_params = self._extract_form_params(soup, stock_code)
            
            if not form_params:
                print("ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ¼ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            basic_info_url = self.base_url + "JJK010030Action.do"
            response = self.session.post(basic_info_url, data=form_params)
            response.raise_for_status()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿æŒ
            self.last_response = response.text
            
            if self.debug:
                with open(f"debug/{stock_code}_step3_basic_info.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
            
            # åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’æŠ½å‡º
            soup = BeautifulSoup(response.text, 'lxml')
            company_info = self._extract_company_info(soup, stock_code)
            
            return company_info
            
        except requests.RequestException as e:
            print(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _extract_form_params(self, soup: BeautifulSoup, stock_code: str) -> Optional[Dict]:
        """
        æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®é·ç§»ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        """
        try:
            def get_input_value(soup, input_name):
                """ç‰¹å®šã®nameå±æ€§ã‚’æŒã¤inputè¦ç´ ã®valueå€¤ã‚’å–å¾—"""
                target = soup.select_one(f'input[name="{input_name}"]')
                if target:
                    return target.get('value', '')
                return ''
            
            # å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åé›†
            params = {
                'BaseJh': get_input_value(soup, 'BaseJh'),
                'lstDspPg': get_input_value(soup, 'lstDspPg'),
                'dspGs': get_input_value(soup, 'dspGs'),
                'souKnsu': get_input_value(soup, 'souKnsu'),
                'sniMtGmnId': get_input_value(soup, 'sniMtGmnId'),
                'dspJnKbn': get_input_value(soup, 'dspJnKbn'),
                'dspJnKmkNo': get_input_value(soup, 'dspJnKmkNo'),
                'mgrCd': stock_code,  # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’æ‰‹å‹•ã§è¨­å®š
                'jjHisiFlg': get_input_value(soup, 'jjHisiFlg')
            }
            
            # æ¤œç´¢çµæœã®æœ€åˆã®éŠ˜æŸ„ã®æƒ…å ±ã‚’å–å¾—ï¼ˆccJjCrpSelKekkLst_st[0]ï¼‰
            result_params = [
                'ccJjCrpSelKekkLst_st[0].eqMgrCd',
                'ccJjCrpSelKekkLst_st[0].eqMgrNm',
                'ccJjCrpSelKekkLst_st[0].szkbuNm',
                'ccJjCrpSelKekkLst_st[0].gyshDspNm',
                'ccJjCrpSelKekkLst_st[0].dspYuKssnKi'
            ]
            
            for param_name in result_params:
                value = get_input_value(soup, param_name)
                if value:
                    params[param_name] = value
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸã‹ç¢ºèª
            if params['BaseJh']:
                if self.debug:
                    print(f"åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º:")
                    for key, value in params.items():
                        if value:
                            print(f"  {key}: {value}")
                return params
            else:
                print("ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ•ã‚©ãƒ¼ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
        except Exception as e:
            print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _extract_company_info(self, soup: BeautifulSoup, stock_code: str) -> Dict:
        """
        åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰
            
        Returns:
            ä¼æ¥­æƒ…å ±ã®è¾æ›¸
        """
        info = {
            'stock_code': stock_code,
            'company_name': None,
            'company_name_en': None,
            'market': None,
            'industry': None,
            'listing_date': None,
            'fiscal_year_end': None,
            'capital': None,
            'shares_outstanding': None,
            'address': None,
            'representative': None,
            'employees': None,
            'average_age': None,
            'average_salary': None
        }
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    # ãƒ©ãƒ™ãƒ«ã¨å€¤ã‚’å–å¾—
                    label = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    
                    # ãƒ©ãƒ™ãƒ«ã«åŸºã¥ã„ã¦æƒ…å ±ã‚’æ ¼ç´
                    if 'ä¼šç¤¾å' in label and 'è‹±æ–‡' not in label:
                        info['company_name'] = value
                    elif 'è‹±æ–‡ä¼šç¤¾å' in label:
                        info['company_name_en'] = value
                    elif 'å¸‚å ´' in label and 'åŒºåˆ†' in label:
                        info['market'] = value
                    elif 'æ¥­ç¨®' in label:
                        info['industry'] = value
                    elif 'ä¸Šå ´æ—¥' in label:
                        info['listing_date'] = value
                    elif 'æ±ºç®—æœŸ' in label:
                        info['fiscal_year_end'] = value
                    elif 'è³‡æœ¬é‡‘' in label:
                        info['capital'] = value
                    elif 'ç™ºè¡Œæ¸ˆæ ªå¼' in label:
                        info['shares_outstanding'] = value
                    elif 'æœ¬ç¤¾æ‰€åœ¨åœ°' in label or 'æ‰€åœ¨åœ°' in label:
                        info['address'] = value
                    elif 'ä»£è¡¨è€…' in label:
                        info['representative'] = value
                    elif 'å¾“æ¥­å“¡æ•°' in label:
                        info['employees'] = value
                    elif 'å¹³å‡å¹´é½¢' in label:
                        info['average_age'] = value
                    elif 'å¹³å‡å¹´å' in label:
                        info['average_salary'] = value
        
        # ä¼æ¥­åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚„ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰æ¢ã™
        if not info['company_name']:
            # h1, h2, h3ã‚¿ã‚°ã‹ã‚‰æ¢ã™
            for heading in soup.find_all(['h1', 'h2', 'h3']):
                text = heading.get_text(strip=True)
                if stock_code in text:
                    # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ä¼æ¥­åã‚’æŠ½å‡º
                    company_text = text.replace(stock_code, '').strip()
                    # æ‹¬å¼§ã‚„è¨˜å·ã‚’é™¤å»
                    company_text = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', company_text).strip()
                    if company_text:
                        info['company_name'] = company_text
                        break
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        if self.debug:
            print(f"æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±:")
            for key, value in info.items():
                if value:
                    print(f"  {key}: {value}")
        
        return info
    
    def fetch_disclosure_documents(self, stock_code: str) -> List[Dict]:
        """
        é©æ™‚é–‹ç¤ºæ›¸é¡ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆXBRLã€HTMLã‚µãƒãƒªãƒ¼ã€PDFç­‰ï¼‰
        
        Args:
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰
            
        Returns:
            é–‹ç¤ºæ›¸é¡ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ã¾ãšä¼æ¥­ã®åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            print(f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {stock_code} ã®é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—ä¸­...")
            
            # æ¤œç´¢ã‹ã‚‰åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã¾ã§é·ç§»
            company_info = self.search_company(stock_code)
            if not company_info:
                print("åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return []
            
            # æœ€å¾Œã«å–å¾—ã—ãŸHTMLã‹ã‚‰é–‹ç¤ºæƒ…å ±ã‚’æŠ½å‡º
            # ï¼ˆsearch_companyã§æ—¢ã«åŸºæœ¬æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—æ¸ˆã¿ï¼‰
            if self.debug:
                html_file = f"debug/{stock_code}_step3_basic_info.html"
                with open(html_file, 'r', encoding='utf-8') as f:
                    html_content = f.read()
            else:
                # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã¯å†åº¦å–å¾—ãŒå¿…è¦
                print("é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’æŠ½å‡ºä¸­...")
                return self._extract_disclosure_from_last_response(stock_code)
            
            soup = BeautifulSoup(html_content, 'lxml')
            return self._extract_disclosure_info(soup)
            
        except Exception as e:
            print(f"é©æ™‚é–‹ç¤ºæƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _extract_disclosure_from_last_response(self, stock_code: str) -> List[Dict]:
        """
        æœ€å¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’æŠ½å‡ºï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        """
        if self.last_response:
            soup = BeautifulSoup(self.last_response, 'lxml')
            return self._extract_disclosure_info(soup)
        return []
    
    def _extract_disclosure_info(self, soup: BeautifulSoup) -> List[Dict]:
        """
        HTMLã‹ã‚‰é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            é–‹ç¤ºæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        import urllib.parse
        
        disclosure_list = []
        
        try:
            # é©æ™‚é–‹ç¤ºæƒ…å ±ã¨ç¸¦è¦§æ›¸é¡ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            target_tables = []
            
            # div.pagecontentsé…ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            table_elms = soup.select('div.pagecontents > table')
            if not table_elms:
                # åˆ¥ã®æ–¹æ³•ã§æ¢ã™
                table_elms = soup.find_all('table')
            
            for table_elm in table_elms:
                table_id = table_elm.get('id', '')
                # closeUpKaiJi* ã¾ãŸã¯ closeUpFili* ã®IDã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
                # ï¼ˆopenã¯é™¤å¤–ï¼‰
                if table_id and ('KaiJi' in table_id or 'Fili' in table_id) and ('open' not in table_id):
                    target_tables.append(table_elm)
                    if self.debug:
                        print(f"é–‹ç¤ºæƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ç™ºè¦‹: {table_id}")
            
            # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            for table_elm in target_tables:
                # å…¥ã‚Œå­ã«ãªã£ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«è¦ç´ ã‚’æ¢ã™
                inner_table = table_elm.select_one('table')
                if not inner_table:
                    continue
                
                # trè¦ç´ ã‚’å‡¦ç†
                tr_elms = inner_table.select('tr')
                for tr_elm in tr_elms:
                    tr_id = tr_elm.get('id')
                    if not tr_id:
                        continue
                    
                    disclosure_info = {
                        'date': '',
                        'title': '',
                        'pdf_url': '',
                        'xbrl_url': '',
                        'html_summary_url': '',
                        'attachments': []
                    }
                    
                    # 1ã¤ç›®ã®tdè¦ç´ : é–‹ç¤ºæ—¥
                    date_td = tr_elm.select_one('td:nth-of-type(1)')
                    if date_td:
                        disclosure_info['date'] = date_td.get_text(strip=True)
                    
                    # 2ã¤ç›®ã®tdè¦ç´ : è¡¨é¡Œã¨PDF URL
                    title_td = tr_elm.select_one('td:nth-of-type(2)')
                    if title_td:
                        a_elm = title_td.select_one('a')
                        if a_elm:
                            disclosure_info['title'] = a_elm.get_text(strip=True)
                            pdf_href = a_elm.get('href', '')
                            if pdf_href:
                                disclosure_info['pdf_url'] = urllib.parse.urljoin(
                                    'https://www2.jpx.co.jp', pdf_href.strip()
                                )
                    
                    # 3ã¤ç›®ã®tdè¦ç´ : XBRL URL
                    xbrl_td = tr_elm.select_one('td:nth-of-type(3)')
                    if xbrl_td:
                        img_elm = xbrl_td.select_one('img')
                        if img_elm:
                            onclick = img_elm.get('onclick', '')
                            if onclick:
                                # ãƒ–ãƒ­ã‚°ã‚’å‚è€ƒã«ã—ãŸXBRL URLæŠ½å‡º
                                # ä¾‹: downloadXbrl2('99840','1','/081220241105608352.zip');
                                onclick_code_str = onclick.replace('\n', '').strip()
                                parts = onclick_code_str.split(',')
                                if len(parts) >= 3:
                                    # æœ€å¾Œã®è¦ç´ ãŒãƒ‘ã‚¹
                                    xbrl_path = parts[-1].strip().replace("'", '').replace(');', '')
                                    if xbrl_path and xbrl_path.startswith('/'):
                                        disclosure_info['xbrl_url'] = f'https://www2.jpx.co.jp/disc{xbrl_path}'
                                        if self.debug:
                                            print(f"XBRL URLæŠ½å‡º: {disclosure_info['xbrl_url']}")
                        
                        # XBRLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åˆ¥ã®è¦ç´ ã‚‚ç¢ºèª
                        if not disclosure_info['xbrl_url']:
                            # aã‚¿ã‚°ã§XBRLãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆ
                            xbrl_link = xbrl_td.select_one('a')
                            if xbrl_link:
                                href = xbrl_link.get('href', '')
                                if href and ('.zip' in href.lower() or 'xbrl' in href.lower()):
                                    disclosure_info['xbrl_url'] = urllib.parse.urljoin(
                                        'https://www2.jpx.co.jp', href
                                    )
                    
                    # 4ã¤ç›®ã®tdè¦ç´ : HTMLã‚µãƒãƒªãƒ¼
                    html_td = tr_elm.select_one('td:nth-of-type(4)')
                    if html_td:
                        # aã‚¿ã‚°ã®hrefå±æ€§ã‹ã‚‰ç›´æ¥URLã‚’å–å¾—
                        html_link = html_td.select_one('a')
                        if html_link:
                            href = html_link.get('href', '')
                            if href and '.htm' in href.lower():
                                disclosure_info['html_summary_url'] = urllib.parse.urljoin(
                                    'https://www2.jpx.co.jp', href
                                )
                                if self.debug:
                                    print(f"HTMLã‚µãƒãƒªãƒ¼URLæŠ½å‡º: {disclosure_info['html_summary_url']}")
                        
                        # hrefå±æ€§ã«ãªã„å ´åˆã¯imgã®onclickå±æ€§ã‚‚ç¢ºèªï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                        elif not disclosure_info['html_summary_url']:
                            img_elm = html_td.select_one('img')
                            if img_elm:
                                onclick = img_elm.get('onclick', '')
                                if onclick and 'window.open' in onclick:
                                    # URLã‚’æŠ½å‡º
                                    import re
                                    url_match = re.search(r"window\.open\('([^']+)'", onclick)
                                    if url_match:
                                        disclosure_info['html_summary_url'] = urllib.parse.urljoin(
                                            'https://www2.jpx.co.jp', url_match.group(1)
                                        )
                    
                    # 5ã¤ç›®ã®tdè¦ç´ : æ·»ä»˜è³‡æ–™
                    attach_td = tr_elm.select_one('td:nth-of-type(5)')
                    if attach_td:
                        attach_links = attach_td.select('a')
                        for link in attach_links:
                            attach_url = link.get('href', '')
                            if attach_url:
                                disclosure_info['attachments'].append(
                                    urllib.parse.urljoin('https://www2.jpx.co.jp', attach_url)
                                )
                    
                    # æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    if disclosure_info['title']:
                        disclosure_list.append(disclosure_info)
                        if self.debug:
                            print(f"é–‹ç¤ºæƒ…å ±æŠ½å‡º: {disclosure_info['title'][:30]}...")
            
            print(f"åˆè¨ˆ {len(disclosure_list)} ä»¶ã®é–‹ç¤ºæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"é–‹ç¤ºæƒ…å ±ã®æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
        
        return disclosure_list
    
    def download_xbrl_files(self, disclosure_docs: List[Dict], download_dir: str = "downloads/xbrl", stock_code: str = "") -> List[Dict]:
        """
        XBRL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            disclosure_docs: é–‹ç¤ºæƒ…å ±ã®ãƒªã‚¹ãƒˆ
            download_dir: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆç”¨ï¼‰
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        import os
        from urllib.parse import urlparse
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(download_dir, exist_ok=True)
        
        download_results = []
        
        # XBRLãŒã‚ã‚‹é–‹ç¤ºæƒ…å ±ã®ã¿ã‚’å‡¦ç†
        xbrl_docs = [doc for doc in disclosure_docs if doc.get('xbrl_url')]
        
        print(f"\n{len(xbrl_docs)}ä»¶ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        for i, doc in enumerate(xbrl_docs, 1):
            try:
                xbrl_url = doc['xbrl_url']
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚ã‹ã‚Šã‚„ã™ã„å½¢å¼ï¼‰
                # æ—¥ä»˜ã‚’ yyyy-mm-dd å½¢å¼ã«å¤‰æ›
                date_formatted = doc['date'].replace('/', '-')
                
                # è¡¨é¡Œã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»
                safe_title = "".join(c for c in doc['title'] if c not in '<>:"/\\|?*').strip()
                
                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼: é–‹ç¤ºæ—¥_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_è¡¨é¡Œ_xbrl.zip
                filename = f"{date_formatted}_{stock_code}_{safe_title}_xbrl.zip"
                filepath = os.path.join(download_dir, filename)
                
                print(f"[{i}/{len(xbrl_docs)}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {doc['title'][:50]}...")
                
                # XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = self.session.get(xbrl_url)
                response.raise_for_status()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                download_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'xbrl_url': xbrl_url,
                    'local_file': filepath,
                    'file_size': len(response.content),
                    'status': 'success'
                }
                
                download_results.append(download_result)
                
                print(f"  â†’ å®Œäº†: {filename} ({len(response.content):,} bytes)")
                
                # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
                if i < len(xbrl_docs):
                    time.sleep(1)
                    
            except Exception as e:
                error_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'xbrl_url': doc.get('xbrl_url', ''),
                    'local_file': None,
                    'file_size': 0,
                    'status': 'error',
                    'error': str(e)
                }
                download_results.append(error_result)
                print(f"  â†’ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ {sum(1 for r in download_results if r['status'] == 'success')} / {len(download_results)} ä»¶")
        
        return download_results
    
    def download_html_summaries(self, stock_code: str) -> List[Dict]:
        """
        HTMLã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        import os
        import time
        import requests
        from urllib.parse import urlparse
        
        # é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—
        disclosure_info = self.fetch_disclosure_documents(stock_code)
        
        # HTMLã‚µãƒãƒªãƒ¼URLãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        html_docs = [doc for doc in disclosure_info if doc.get('html_summary_url')]
        
        if not html_docs:
            print("HTMLã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        download_dir = os.path.join('downloads', 'html_summary', stock_code)
        os.makedirs(download_dir, exist_ok=True)
        
        print(f"\nHTMLã‚µãƒãƒªãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(html_docs)} ä»¶")
        print(f"ä¿å­˜å…ˆ: {download_dir}")
        
        download_results = []
        
        for i, doc in enumerate(html_docs, 1):
            try:
                html_url = doc['html_summary_url']
                print(f"[{i}/{len(html_docs)}] {doc['title'][:50]}...")
                print(f"  URL: {html_url}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚ã‹ã‚Šã‚„ã™ã„å½¢å¼ï¼‰
                # æ—¥ä»˜ã‚’ yyyy-mm-dd å½¢å¼ã«å¤‰æ›
                date_formatted = doc['date'].replace('/', '-')
                
                # è¡¨é¡Œã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»
                safe_title = "".join(c for c in doc['title'] if c not in '<>:"/\\|?*').strip()
                
                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼: é–‹ç¤ºæ—¥_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_è¡¨é¡Œ_summary.htm
                filename = f"{date_formatted}_{stock_code}_{safe_title}_summary.htm"
                
                local_file = os.path.join(download_dir, filename)
                
                # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if os.path.exists(local_file):
                    print(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—: ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")
                    download_result = {
                        'title': doc['title'],
                        'date': doc['date'],
                        'html_summary_url': html_url,
                        'local_file': local_file,
                        'file_size': os.path.getsize(local_file),
                        'status': 'skipped'
                    }
                    download_results.append(download_result)
                    continue
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                response = requests.get(html_url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                with open(local_file, 'wb') as f:
                    f.write(response.content)
                
                download_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'html_summary_url': html_url,
                    'local_file': local_file,
                    'file_size': len(response.content),
                    'status': 'success'
                }
                
                download_results.append(download_result)
                
                print(f"  â†’ å®Œäº†: {filename} ({len(response.content):,} bytes)")
                
                # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
                if i < len(html_docs):
                    time.sleep(1)
                    
            except Exception as e:
                error_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'html_summary_url': doc.get('html_summary_url', ''),
                    'local_file': None,
                    'file_size': 0,
                    'status': 'error',
                    'error': str(e)
                }
                download_results.append(error_result)
                print(f"  â†’ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\nHTMLã‚µãƒãƒªãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ {sum(1 for r in download_results if r['status'] == 'success')} / {len(download_results)} ä»¶")
        
        return download_results
    
    def download_attachments(self, stock_code: str) -> List[Dict]:
        """
        æ·»ä»˜è³‡æ–™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            stock_code: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã®ãƒªã‚¹ãƒˆ
        """
        import os
        import time
        import requests
        from urllib.parse import urlparse
        
        # é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—
        disclosure_info = self.fetch_disclosure_documents(stock_code)
        
        # æ·»ä»˜è³‡æ–™URLãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        attachment_docs = []
        for doc in disclosure_info:
            if doc.get('attachments'):
                for attach_url in doc['attachments']:
                    attachment_docs.append({
                        'title': doc['title'],
                        'date': doc['date'],
                        'attachment_url': attach_url
                    })
        
        if not attachment_docs:
            print("æ·»ä»˜è³‡æ–™ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        download_dir = os.path.join('downloads', 'attachments', stock_code)
        os.makedirs(download_dir, exist_ok=True)
        
        print(f"\næ·»ä»˜è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(attachment_docs)} ä»¶")
        print(f"ä¿å­˜å…ˆ: {download_dir}")
        
        download_results = []
        
        for i, doc in enumerate(attachment_docs, 1):
            try:
                attach_url = doc['attachment_url']
                print(f"[{i}/{len(attachment_docs)}] {doc['title'][:50]}...")
                print(f"  URL: {attach_url}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚ã‹ã‚Šã‚„ã™ã„å½¢å¼ï¼‰
                # æ—¥ä»˜ã‚’ yyyy-mm-dd å½¢å¼ã«å¤‰æ›
                date_formatted = doc['date'].replace('/', '-')
                
                # è¡¨é¡Œã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»
                safe_title = "".join(c for c in doc['title'] if c not in '<>:"/\\|?*').strip()
                
                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼: é–‹ç¤ºæ—¥_è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_è¡¨é¡Œ_attachments.htm
                filename = f"{date_formatted}_{stock_code}_{safe_title}_attachments.htm"
                
                local_file = os.path.join(download_dir, filename)
                
                # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if os.path.exists(local_file):
                    print(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—: ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")
                    download_result = {
                        'title': doc['title'],
                        'date': doc['date'],
                        'attachment_url': attach_url,
                        'local_file': local_file,
                        'file_size': os.path.getsize(local_file),
                        'status': 'skipped'
                    }
                    download_results.append(download_result)
                    continue
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                response = requests.get(attach_url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                with open(local_file, 'wb') as f:
                    f.write(response.content)
                
                download_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'attachment_url': attach_url,
                    'local_file': local_file,
                    'file_size': len(response.content),
                    'status': 'success'
                }
                
                download_results.append(download_result)
                
                print(f"  â†’ å®Œäº†: {filename} ({len(response.content):,} bytes)")
                
                # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
                if i < len(attachment_docs):
                    time.sleep(1)
                    
            except Exception as e:
                error_result = {
                    'title': doc['title'],
                    'date': doc['date'],
                    'attachment_url': doc.get('attachment_url', ''),
                    'local_file': None,
                    'file_size': 0,
                    'status': 'error',
                    'error': str(e)
                }
                download_results.append(error_result)
                print(f"  â†’ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\næ·»ä»˜è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ {sum(1 for r in download_results if r['status'] == 'success')} / {len(download_results)} ä»¶")
        
        return download_results
    
    def download_all_files_batch(self, codelist_csv: str = "codelist.csv", download_types: list = None, 
                                 resume_from: int = 0, max_companies: int = None, 
                                 delay_seconds: int = 3) -> Dict:
        """
        codelist.csvã‹ã‚‰å…¨éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            codelist_csv: éŠ˜æŸ„ãƒªã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            download_types: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡ ['xbrl', 'html', 'attachments'] (Noneã§å…¨ç¨®é¡)
            resume_from: å†é–‹ã™ã‚‹è¡Œç•ªå·ï¼ˆ0ã‹ã‚‰é–‹å§‹ï¼‰
            max_companies: æœ€å¤§å‡¦ç†ä¼æ¥­æ•°ï¼ˆNoneã§å…¨ä¼æ¥­ï¼‰
            delay_seconds: ä¼æ¥­é–“ã®å¾…æ©Ÿç§’æ•°
            
        Returns:
            å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
        """
        import csv
        import time
        from datetime import datetime
        import os
        
        if download_types is None:
            download_types = ['xbrl', 'html', 'attachments']
        
        print(f"ğŸš€ codelist.csvã‹ã‚‰å…¨éŠ˜æŸ„ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ğŸ“‹ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {codelist_csv}")
        print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡: {', '.join(download_types)}")
        print(f"â±ï¸  ä¼æ¥­é–“å¾…æ©Ÿæ™‚é–“: {delay_seconds}ç§’")
        print(f"ğŸ”„ å†é–‹ä½ç½®: {resume_from}è¡Œç›®ã‹ã‚‰")
        print("-" * 60)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        companies = []
        try:
            with open(codelist_csv, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for i, row in enumerate(reader):
                    if i >= resume_from:
                        if max_companies and len(companies) >= max_companies:
                            break
                        companies.append({
                            'row_number': i + 1,  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è€ƒæ…®
                            'stock_code': row['code'],
                            'company_name': row['éŠ˜æŸ„å'],
                            'industry': row['æ¥­ç¨®'],
                            'topix_weight': row['TOPIXã«å ã‚ã‚‹å€‹åˆ¥éŠ˜æŸ„ã®ã‚¦ã‚¨ã‚¤ãƒˆ'],
                            'index_category': row['ãƒ‹ãƒ¥ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒºåˆ†']
                        })
        except FileNotFoundError:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {codelist_csv} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {'status': 'error', 'message': 'CSV file not found'}
        except Exception as e:
            print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {'status': 'error', 'message': str(e)}
        
        print(f"ğŸ“Š å‡¦ç†å¯¾è±¡ä¼æ¥­æ•°: {len(companies)} ç¤¾")
        
        # çµæœè¨˜éŒ²ç”¨
        batch_results = {
            'start_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_companies': len(companies),
            'processed_companies': 0,
            'successful_companies': 0,
            'failed_companies': 0,
            'download_types': download_types,
            'results': [],
            'failed_companies_list': [],
            'statistics': {}
        }
        
        # å‡¦ç†é–‹å§‹
        for i, company in enumerate(companies, 1):
            stock_code = company['stock_code']
            company_name = company['company_name']
            row_number = company['row_number']
            
            print(f"\n[{i}/{len(companies)}] å‡¦ç†ä¸­: {company_name} ({stock_code}) - è¡Œç•ªå·: {row_number}")
            
            company_result = {
                'row_number': row_number,
                'stock_code': stock_code,
                'company_name': company_name,
                'industry': company['industry'],
                'downloads': {},
                'total_files': 0,
                'success_files': 0,
                'status': 'processing'
            }
            
            try:
                # é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—
                disclosure_info = self.fetch_disclosure_documents(stock_code)
                
                if not disclosure_info:
                    print(f"  âš ï¸  é©æ™‚é–‹ç¤ºæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    company_result['status'] = 'no_disclosure_data'
                    batch_results['results'].append(company_result)
                    batch_results['failed_companies_list'].append({
                        'stock_code': stock_code,
                        'company_name': company_name,
                        'reason': 'no_disclosure_data'
                    })
                    continue
                
                print(f"  ğŸ“‹ é©æ™‚é–‹ç¤ºæƒ…å ±: {len(disclosure_info)} ä»¶å–å¾—")
                
                # å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                for download_type in download_types:
                    try:
                        if download_type == 'xbrl':
                            results = self.download_xbrl_files(disclosure_info, f"downloads/xbrl/{stock_code}", stock_code)
                        elif download_type == 'html':
                            results = self.download_html_summaries(stock_code)
                        elif download_type == 'attachments':
                            results = self.download_attachments(stock_code)
                        else:
                            continue
                        
                        # çµæœé›†è¨ˆ
                        total_files = len(results)
                        success_files = sum(1 for r in results if r['status'] == 'success')
                        
                        company_result['downloads'][download_type] = {
                            'total': total_files,
                            'success': success_files,
                            'failed': total_files - success_files
                        }
                        
                        company_result['total_files'] += total_files
                        company_result['success_files'] += success_files
                        
                        print(f"    ğŸ“¥ {download_type.upper()}: {success_files}/{total_files} ä»¶æˆåŠŸ")
                        
                    except Exception as download_error:
                        print(f"    âŒ {download_type.upper()}ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {download_error}")
                        company_result['downloads'][download_type] = {
                            'total': 0,
                            'success': 0,
                            'failed': 0,
                            'error': str(download_error)
                        }
                
                # ä¼æ¥­ã®å‡¦ç†çµæœã‚’åˆ¤å®š
                if company_result['success_files'] > 0:
                    company_result['status'] = 'success'
                    batch_results['successful_companies'] += 1
                    print(f"  âœ… æˆåŠŸ: åˆè¨ˆ {company_result['success_files']}/{company_result['total_files']} ãƒ•ã‚¡ã‚¤ãƒ«")
                else:
                    company_result['status'] = 'failed'
                    batch_results['failed_companies'] += 1
                    batch_results['failed_companies_list'].append({
                        'stock_code': stock_code,
                        'company_name': company_name,
                        'reason': 'no_successful_downloads'
                    })
                    print(f"  âŒ å¤±æ•—: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
                
            except Exception as e:
                print(f"  âŒ ä¼æ¥­å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                company_result['status'] = 'error'
                company_result['error'] = str(e)
                batch_results['failed_companies'] += 1
                batch_results['failed_companies_list'].append({
                    'stock_code': stock_code,
                    'company_name': company_name,
                    'reason': f'processing_error: {str(e)}'
                })
            
            batch_results['results'].append(company_result)
            batch_results['processed_companies'] += 1
            
            # é€²æ—è¡¨ç¤º
            progress = (i / len(companies)) * 100
            print(f"  ğŸ“Š é€²æ—: {progress:.1f}% ({i}/{len(companies)})")
            
            # å¾…æ©Ÿï¼ˆæœ€å¾Œã®ä¼æ¥­ä»¥å¤–ï¼‰
            if i < len(companies):
                print(f"  â³ {delay_seconds}ç§’å¾…æ©Ÿä¸­...")
                time.sleep(delay_seconds)
        
        # å‡¦ç†å®Œäº†
        batch_results['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        batch_results['status'] = 'completed'
        
        # çµ±è¨ˆæƒ…å ±ã®ç”Ÿæˆ
        batch_results['statistics'] = self._generate_batch_statistics(batch_results)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
        self._save_batch_report(batch_results)
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self._display_batch_summary(batch_results)
        
        return batch_results
    
    def _generate_batch_statistics(self, batch_results: Dict) -> Dict:
        """ãƒãƒƒãƒå‡¦ç†ã®çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        stats = {
            'success_rate': 0,
            'total_files_downloaded': 0,
            'industry_breakdown': {},
            'download_type_stats': {}
        }
        
        if batch_results['processed_companies'] > 0:
            stats['success_rate'] = (batch_results['successful_companies'] / batch_results['processed_companies']) * 100
        
        for result in batch_results['results']:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆè¨ˆ
            stats['total_files_downloaded'] += result.get('success_files', 0)
            
            # æ¥­ç¨®åˆ¥çµ±è¨ˆ
            industry = result.get('industry', 'ä¸æ˜')
            if industry not in stats['industry_breakdown']:
                stats['industry_breakdown'][industry] = {'total': 0, 'success': 0}
            
            stats['industry_breakdown'][industry]['total'] += 1
            if result['status'] == 'success':
                stats['industry_breakdown'][industry]['success'] += 1
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡åˆ¥çµ±è¨ˆ
            for download_type, download_stats in result.get('downloads', {}).items():
                if download_type not in stats['download_type_stats']:
                    stats['download_type_stats'][download_type] = {'total': 0, 'success': 0}
                
                stats['download_type_stats'][download_type]['total'] += download_stats.get('total', 0)
                stats['download_type_stats'][download_type]['success'] += download_stats.get('success', 0)
        
        return stats
    
    def _save_batch_report(self, batch_results: Dict):
        """ãƒãƒƒãƒå‡¦ç†çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        import json
        import os
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"data/batch_download_report_{timestamp}.json"
        
        os.makedirs("data", exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_file}")
    
    def _display_batch_summary(self, batch_results: Dict):
        """ãƒãƒƒãƒå‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print(f"\n{'='*60}")
        print(f"ğŸ‰ å…¨éŠ˜æŸ„ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
        print(f"{'='*60}")
        print(f"â° å‡¦ç†æ™‚é–“: {batch_results['start_time']} ï½ {batch_results['end_time']}")
        print(f"ğŸ¢ å‡¦ç†ä¼æ¥­æ•°: {batch_results['processed_companies']} ç¤¾")
        print(f"âœ… æˆåŠŸä¼æ¥­æ•°: {batch_results['successful_companies']} ç¤¾")
        print(f"âŒ å¤±æ•—ä¼æ¥­æ•°: {batch_results['failed_companies']} ç¤¾")
        print(f"ğŸ“Š æˆåŠŸç‡: {batch_results['statistics']['success_rate']:.1f}%")
        print(f"ğŸ“ ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {batch_results['statistics']['total_files_downloaded']} ä»¶")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡åˆ¥çµ±è¨ˆ
        print(f"\nğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¨®é¡åˆ¥çµ±è¨ˆ:")
        for download_type, stats in batch_results['statistics']['download_type_stats'].items():
            success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
            print(f"  {download_type.upper()}: {stats['success']}/{stats['total']} ä»¶ ({success_rate:.1f}%)")
        
        # å¤±æ•—ä¼æ¥­ä¸€è¦§ï¼ˆæœ€åˆã®10ä»¶ã®ã¿ï¼‰
        if batch_results['failed_companies_list']:
            print(f"\nâŒ å¤±æ•—ä¼æ¥­ä¸€è¦§ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
            for i, failed in enumerate(batch_results['failed_companies_list'][:10], 1):
                print(f"  {i}. {failed['company_name']} ({failed['stock_code']}) - {failed['reason']}")
            
            if len(batch_results['failed_companies_list']) > 10:
                print(f"  ... ä»– {len(batch_results['failed_companies_list']) - 10} ä»¶")
        
        print(f"{'='*60}")